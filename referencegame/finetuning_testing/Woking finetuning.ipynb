{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cb8b8929",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import math\n",
    "import json\n",
    "from copy import deepcopy\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "# QLoRA / quantization imports\n",
    "from transformers import BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "import wandb\n",
    "from huggingface_hub import login\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "701bb3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/project/hf_api_login_key.txt', 'r') as f:\n",
    "    hf_api = f.read()\n",
    "login(token=hf_api)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "308072bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------\n",
    "# put these at the VERY TOP of your script, before importing HF libs\n",
    "# ------------------------------------------------------------------\n",
    "import os\n",
    "os.environ[\"HF_HOME\"] = \"/SERVER_FAST_SSD/hf\"\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = \"/SERVER_FAST_SSD/hf/transformers\"\n",
    "os.environ[\"HF_DATASETS_CACHE\"] = \"/SERVER_FAST_SSD/hf/datasets\"\n",
    "os.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"/SERVER_FAST_SSD/hf/hub\"\n",
    "os.environ[\"WANDB_DISABLED\"] = \"false\"   # flip to \"false\" if you actually want wandb\n",
    "\n",
    "OUTPUT_DIR = \"/project/testing_finetuning\"  # your requested path\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e6075e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 0) Load data\n",
    "# ---------------------------\n",
    "with open('./referencegame_data_DS.json', 'r') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2ec1b772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------\n",
    "# 1) Dataset + collator\n",
    "# -----------------------------------------\n",
    "class BanditDataset(Dataset):\n",
    "    def __init__(self, rows: List[dict], tokenizer: AutoTokenizer, max_len: int = 4096):\n",
    "        self.rows = rows\n",
    "        self.tok = tokenizer\n",
    "        if self.tok.pad_token is None:\n",
    "            self.tok.pad_token = self.tok.eos_token\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.rows)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        r = self.rows[i]\n",
    "        if r[\"role\"] == \"gen\":\n",
    "            prompt, action = r[\"prompt_speaker\"], r[\"utterance\"]\n",
    "        else:\n",
    "            prompt, action = r[\"prompt_listener\"], r[\"guess\"]\n",
    "\n",
    "        enc_p = self.tok(prompt, add_special_tokens=False)\n",
    "        enc_a = self.tok(action, add_special_tokens=False)\n",
    "\n",
    "        input_ids = enc_p[\"input_ids\"] + enc_a[\"input_ids\"]\n",
    "        labels = [-100] * len(enc_p[\"input_ids\"]) + enc_a[\"input_ids\"]\n",
    "        attn = [1] * len(input_ids)\n",
    "\n",
    "        if len(input_ids) > self.max_len:\n",
    "            overflow = len(input_ids) - self.max_len\n",
    "            input_ids = input_ids[overflow:]\n",
    "            labels = labels[overflow:]\n",
    "            attn = attn[overflow:]\n",
    "\n",
    "        logp_behavior = r.get(\"logp_behavior\", None)\n",
    "        has_behav = float((logp_behavior is not None) and (r[\"reward\"] == -1))\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(input_ids, dtype=torch.long),\n",
    "            \"attention_mask\": torch.tensor(attn, dtype=torch.long),\n",
    "            \"labels\": torch.tensor(labels, dtype=torch.long),\n",
    "            \"reward\": torch.tensor(float(r[\"reward\"]), dtype=torch.float),\n",
    "            \"logp_behavior\": torch.tensor(0.0 if logp_behavior is None else float(logp_behavior), dtype=torch.float),\n",
    "            \"has_behav\": torch.tensor(has_behav, dtype=torch.float),\n",
    "        }\n",
    "\n",
    "class PadCollator:\n",
    "    def __init__(self, pad_id: int):\n",
    "        self.pad_id = pad_id\n",
    "\n",
    "    def __call__(self, batch: List[Dict[str, torch.Tensor]]) -> Dict[str, torch.Tensor]:\n",
    "        max_len = max(x[\"input_ids\"].shape[0] for x in batch)\n",
    "        for x in batch:\n",
    "            pad = max_len - x[\"input_ids\"].shape[0]\n",
    "            if pad > 0:\n",
    "                x[\"input_ids\"] = torch.nn.functional.pad(x[\"input_ids\"], (0, pad), value=self.pad_id)\n",
    "                x[\"attention_mask\"] = torch.nn.functional.pad(x[\"attention_mask\"], (0, pad), value=0)\n",
    "                x[\"labels\"] = torch.nn.functional.pad(x[\"labels\"], (0, pad), value=-100)\n",
    "        out = {k: torch.stack([x[k] for x in batch]) for k in [\"input_ids\", \"attention_mask\", \"labels\", \"reward\", \"logp_behavior\", \"has_behav\"]}\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6e92324c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ips_reinforce_loss(model, batch, ips_clip: float = 5.0) -> torch.Tensor:\n",
    "    out = model(input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"], return_dict=True)\n",
    "    logprobs = torch.log_softmax(out.logits, dim=-1)\n",
    "    labels = batch[\"labels\"]\n",
    "    mask = (labels != -100)\n",
    "\n",
    "    # sequence log-prob\n",
    "    labels_safe = torch.where(mask, labels, torch.zeros_like(labels))\n",
    "    tok_lp = torch.gather(logprobs, -1, labels_safe.unsqueeze(-1)).squeeze(-1)\n",
    "    seq_logp = (tok_lp * mask).sum(dim=1)\n",
    "\n",
    "    reward     = batch[\"reward\"].to(seq_logp.dtype)\n",
    "    logp_behav = batch[\"logp_behavior\"].to(seq_logp.dtype)\n",
    "    has_behav  = (batch[\"has_behav\"] > 0.5)\n",
    "\n",
    "    # importance ratio\n",
    "    with torch.no_grad():\n",
    "        print(seq_logp)\n",
    "        print(logp_behav)\n",
    "        diff   = seq_logp.detach() - logp_behav\n",
    "        ratio  = torch.exp(diff)\n",
    "        if ips_clip is not None and ips_clip > 0:\n",
    "            ratio = torch.clamp(ratio, max=ips_clip)\n",
    "        # if no behavior policy, set c=1\n",
    "        c = torch.where((reward < 0) & has_behav, ratio, torch.ones_like(ratio))\n",
    "        print(c)\n",
    "\n",
    "    # final loss\n",
    "    per_ex = - c * reward * seq_logp\n",
    "    loss = per_ex.mean()\n",
    "    return torch.nan_to_num(loss, nan=0.0, neginf=0.0, posinf=0.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "431aad6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def ips_reinforce_loss(model, batch) -> torch.Tensor:\n",
    "#     out = model(\n",
    "#         input_ids=batch[\"input_ids\"],\n",
    "#         attention_mask=batch[\"attention_mask\"],\n",
    "#         return_dict=True,\n",
    "#     )\n",
    "#     logits = out.logits  # [B,T,V]\n",
    "#     logprobs = torch.log_softmax(logits, dim=-1)\n",
    "#     labels = batch[\"labels\"]                     # [B,T]\n",
    "\n",
    "#     mask = (labels != -100)\n",
    "#     labels_safe = torch.where(mask, labels, torch.zeros_like(labels))\n",
    "#     token_lp = torch.gather(logprobs, dim=-1, index=labels_safe.unsqueeze(-1)).squeeze(-1)\n",
    "#     token_lp = token_lp * mask\n",
    "#     logp_curr = token_lp.sum(dim=1)              # [B]\n",
    "\n",
    "#     reward = batch[\"reward\"]\n",
    "#     logp_behav = batch[\"logp_behavior\"]\n",
    "#     has_behav = (batch[\"has_behav\"] > 0.5)\n",
    "\n",
    "#     ratio = torch.exp(logp_curr - logp_behav)\n",
    "#     c = torch.where((reward < 0) & has_behav, ratio, torch.ones_like(ratio))\n",
    "#     c = c.detach()                               # do not backprop through c\n",
    "\n",
    "#     per_ex = -(reward * c * logp_curr)\n",
    "#     keep = ~((reward < 0) & (~has_behav))        # drop negatives w/o behavior prob\n",
    "#     if keep.sum() == 0:\n",
    "#         # avoid NaN if the whole batch is filtered\n",
    "#         return per_ex.mean() * 0.0\n",
    "#     return per_ex[keep].mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7f580349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only reinforc\n",
    "def reinforce_loss(model, batch) -> torch.Tensor:\n",
    "    out = model(\n",
    "        input_ids=batch[\"input_ids\"],\n",
    "        attention_mask=batch[\"attention_mask\"],\n",
    "        return_dict=True,\n",
    "    )\n",
    "    logprobs = torch.log_softmax(out.logits, dim=-1)\n",
    "    labels = batch[\"labels\"]                      # [B, T]\n",
    "    mask = (labels != -100)\n",
    "\n",
    "    # sequence log-prob of the action (sum over action tokens only)\n",
    "    labels_safe = torch.where(mask, labels, torch.zeros_like(labels))\n",
    "    tok_lp = torch.gather(logprobs, -1, labels_safe.unsqueeze(-1)).squeeze(-1)\n",
    "    seq_logp = (tok_lp * mask).sum(dim=1)        # [B], typically ≤ 0\n",
    "\n",
    "    reward = batch[\"reward\"].to(seq_logp.dtype)  # ±1\n",
    "\n",
    "    # (optional) tiny stabilizers while debugging:\n",
    "    seq_logp = torch.nan_to_num(seq_logp, nan=0.0, neginf=-1e4, posinf=0.0)\n",
    "    # Or use per-token average instead of sum:\n",
    "    # seq_len = mask.sum(dim=1).clamp(min=1).float()\n",
    "    # seq_logp = (seq_logp / seq_len).clamp(min=-5.0, max=0.0)\n",
    "\n",
    "    per_ex = -(reward * seq_logp)\n",
    "    loss = per_ex.mean()\n",
    "    return torch.nan_to_num(loss, nan=0.0, neginf=0.0, posinf=0.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "197bcde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------\n",
    "# 6) Trainer subclass to use our loss\n",
    "# -----------------------------------------\n",
    "class IPSReinforceTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        # kwargs may contain num_items_in_batch etc. — we don't use them.\n",
    "        loss = ips_reinforce_loss(model, inputs)\n",
    "        return (loss, {\"loss\": loss}) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e480dcbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# -----------------------------------------\\n# Trainer subclass using plain REINFORCE\\n# -----------------------------------------\\nclass ReinforceTrainer(Trainer):\\n    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\\n        loss = reinforce_loss(model, inputs)\\n        return (loss, {\"loss\": loss}) if return_outputs else loss\\n'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"# -----------------------------------------\n",
    "# Trainer subclass using plain REINFORCE\n",
    "# -----------------------------------------\n",
    "class ReinforceTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        loss = reinforce_loss(model, inputs)\n",
    "        return (loss, {\"loss\": loss}) if return_outputs else loss\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c452930b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tokenizer(model_ckpt: str):\n",
    "    tok = AutoTokenizer.from_pretrained(model_ckpt, use_fast=True)\n",
    "    if tok.pad_token is None:\n",
    "        tok.pad_token = tok.eos_token\n",
    "    return tok\n",
    "\n",
    "def _bf16_supported() -> bool:\n",
    "    if not torch.cuda.is_available():\n",
    "        return False\n",
    "    major, minor = torch.cuda.get_device_capability()\n",
    "    return major >= 8  # Ampere (8.0) or newer\n",
    "\n",
    "def load_qlora_base(model_ckpt: str):\n",
    "    compute_dtype = torch.bfloat16 if _bf16_supported() else torch.float16\n",
    "\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_compute_dtype=compute_dtype,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "    )\n",
    "    \n",
    "    if not torch.cuda.is_available():\n",
    "        raise RuntimeError(\"QLoRA 4-bit requires CUDA; no GPU visible.\")\n",
    "    device_index = torch.cuda.current_device()  # usually 0 on your A100 box\n",
    "    device_map = {\"\": 0}\n",
    "    #device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_ckpt,\n",
    "        quantization_config=bnb_config,\n",
    "        #device_map=\"auto\",\n",
    "        device_map=device_map,\n",
    "        #device_map=device,\n",
    "        attn_implementation=\"eager\",  # more stable while debugging; switch to \"flash_attention_2\" later\n",
    "    )\n",
    "    model.config.use_cache = False\n",
    "    try:\n",
    "        model.gradient_checkpointing_enable(gradient_checkpointing_kwargs={\"use_reentrant\": False})\n",
    "    except TypeError:\n",
    "        model.gradient_checkpointing_enable()\n",
    "\n",
    "    # Prepare for k-bit training (fix layer norms, input grads, etc.)\n",
    "    from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
    "    try:\n",
    "        model = prepare_model_for_kbit_training(\n",
    "            model, use_gradient_checkpointing=True,\n",
    "            gradient_checkpointing_kwargs={\"use_reentrant\": False}\n",
    "        )\n",
    "    except TypeError:\n",
    "        model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=True)\n",
    "\n",
    "    peft_config = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=8,\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\"],  # only these, per paper\n",
    "    )\n",
    "    \n",
    "    model = get_peft_model(model, peft_config)\n",
    "    return model\n",
    "\n",
    "def main():\n",
    "    # --- 0) Set seed\n",
    "    random.seed(9)\n",
    "    \n",
    "    # --- 1) Build/prepare data as before ---\n",
    "\n",
    "    # shuffle data and split into train and dev\n",
    "    train_data, dev_data = train_test_split(data, test_size=0.15, random_state=42, shuffle=True)\n",
    "    \n",
    "    # --- 2) Tokenizer & QLoRA model ---\n",
    "    model_ckpt = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "    tokenizer = load_tokenizer(model_ckpt)\n",
    "    model = load_qlora_base(model_ckpt)\n",
    "\n",
    "    # --- 3) Dataset/Collator ---\n",
    "    dataset = BanditDataset(train_data, tokenizer, max_len=4096)\n",
    "    data_collator = PadCollator(pad_id=tokenizer.pad_token_id)\n",
    "\n",
    "    # --- 4) Trainer args for QLoRA ---\n",
    "    # Use a higher LR typical for LoRA adapters; use paged optimizers for 4-bit\n",
    "    # prefer bf16 on Ampere+, else fp16\n",
    "    use_bf16 = _bf16_supported()\n",
    "    args = TrainingArguments(\n",
    "        output_dir=OUTPUT_DIR,\n",
    "        per_device_train_batch_size=32,\n",
    "        gradient_accumulation_steps=1,\n",
    "        learning_rate=0.0001,#2e-4,\n",
    "        num_train_epochs=1,\n",
    "        lr_scheduler_type=\"constant\",\n",
    "        warmup_ratio=0.0,\n",
    "        weight_decay=0.1,\n",
    "        logging_steps=1,\n",
    "        save_steps=100,\n",
    "        save_total_limit=2,\n",
    "        report_to=\"wandb\",\n",
    "        run_name=\"whyareurunnin\",\n",
    "        bf16=use_bf16,\n",
    "        fp16=not use_bf16,                 # <- only one of these should be True\n",
    "        optim=\"adamw_torch\", #\"paged_adamw_8bit\",\n",
    "        remove_unused_columns=False,\n",
    "        )\n",
    "    \n",
    "    trainer = IPSReinforceTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=dataset,\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "        )\n",
    "\n",
    "    print('Start training')\n",
    "    trainer.train()\n",
    "    \n",
    "    # Save explicitly\n",
    "    print('Save the model')\n",
    "    trainer.save_model()               # saves PEFT adapter weights\n",
    "    tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "\n",
    "    # Later for inference:\n",
    "    # from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "    # from peft import PeftModel\n",
    "\n",
    "    # base = AutoModelForCausalLM.from_pretrained(\n",
    "    #     \"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "    #     quantization_config=BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    #                                            bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\"),\n",
    "    #     device_map=\"auto\",\n",
    "    # )\n",
    "    # model = PeftModel.from_pretrained(base, OUTPUT_DIR)\n",
    "    # tok = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\")\n",
    "\n",
    "\n",
    "    # push adapters\n",
    "    model.push_to_hub(\"imge/llama_v1\")\n",
    "    tokenizer.push_to_hub(\"imge/llama_v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "aa8d737b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13c1d2c2e0d4441bbb355559e09f824a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training\n",
      "tensor([-258.4668, -233.5099, -210.8028,  -53.1822, -277.1786,  -34.8920,\n",
      "        -253.8637,  -35.4692,  -92.0188,  -48.6660, -199.8020, -585.5114,\n",
      "         -55.7941, -253.8636,  -38.1408,  -50.2399,  -52.5271,  -54.4470,\n",
      "        -199.8020, -272.1948,  -51.8451, -258.4668,  -55.1483,  -48.4044,\n",
      "        -123.5749, -180.5758, -382.9168, -667.1732,  -50.0904, -129.7884,\n",
      "        -120.1087,  -53.8062], device='cuda:0', grad_fn=<SumBackward1>)\n",
      "tensor([   0.0000,    0.0000, -207.0000,  -56.7500,    0.0000,    0.0000,\n",
      "        -251.0000,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000,\n",
      "           0.0000, -251.0000,    0.0000,  -54.2500,    0.0000,    0.0000,\n",
      "           0.0000, -266.0000,    0.0000,    0.0000,  -57.0000,    0.0000,\n",
      "           0.0000, -178.0000, -378.0000,    0.0000,    0.0000,    0.0000,\n",
      "           0.0000,  -56.0000], device='cuda:0')\n",
      "tensor([1.0000e+00, 1.0000e+00, 2.2308e-02, 5.0000e+00, 1.0000e+00, 1.0000e+00,\n",
      "        5.7059e-02, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
      "        1.0000e+00, 5.7060e-02, 1.0000e+00, 5.0000e+00, 1.0000e+00, 1.0000e+00,\n",
      "        1.0000e+00, 2.0400e-03, 1.0000e+00, 1.0000e+00, 5.0000e+00, 1.0000e+00,\n",
      "        1.0000e+00, 7.6094e-02, 7.3222e-03, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
      "        1.0000e+00, 5.0000e+00], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='14' max='21' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [14/21 00:42 < 00:24, 0.28 it/s, Epoch 0.62/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>78.219400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>-101.283000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>-25.272900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>85.377300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>-135.862100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>4.443100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>-69.090500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>41.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>102.309500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>108.452000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>53.343500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>21.727700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-505.1823, -559.5725,  -53.2788, -252.4946,  -55.6971, -662.4315,\n",
      "        -111.2572, -252.6646, -663.4072, -159.8318,  -51.9672,  -52.2444,\n",
      "         -55.2432, -244.9593,  -49.4636,  -50.1026,  -53.5878, -258.0309,\n",
      "         -53.4318, -379.8980, -588.1936, -722.5264,  -51.1584, -562.0634,\n",
      "         -54.6298,  -53.1316, -252.5452, -221.4471, -181.6573, -248.1126,\n",
      "         -55.7697,  -50.7195], device='cuda:0', grad_fn=<SumBackward1>)\n",
      "tensor([-492.0000, -552.0000,    0.0000, -251.0000,  -56.2500, -664.0000,\n",
      "           0.0000,    0.0000, -644.0000,    0.0000,  -55.0000,  -55.0000,\n",
      "         -58.0000,    0.0000,  -51.0000,  -55.0000,    0.0000,    0.0000,\n",
      "         -55.0000, -378.0000, -580.0000,    0.0000,    0.0000, -552.0000,\n",
      "         -55.5000,    0.0000, -251.0000,    0.0000,    0.0000, -249.0000,\n",
      "           0.0000,    0.0000], device='cuda:0')\n",
      "tensor([1.8837e-06, 5.1440e-04, 1.0000e+00, 2.2435e-01, 1.7383e+00, 4.7994e+00,\n",
      "        1.0000e+00, 1.0000e+00, 3.7286e-09, 1.0000e+00, 5.0000e+00, 5.0000e+00,\n",
      "        5.0000e+00, 1.0000e+00, 4.6478e+00, 5.0000e+00, 1.0000e+00, 1.0000e+00,\n",
      "        4.7981e+00, 1.4987e-01, 2.7642e-04, 1.0000e+00, 1.0000e+00, 4.2613e-05,\n",
      "        2.3874e+00, 1.0000e+00, 2.1326e-01, 1.0000e+00, 1.0000e+00, 2.4288e+00,\n",
      "        1.0000e+00, 1.0000e+00], device='cuda:0')\n",
      "tensor([-438.9620,  -38.3970,  -54.1552, -557.5323,  -55.9908, -587.7356,\n",
      "         -36.1516,  -52.1243,  -53.8532, -196.8512, -251.8096, -379.8926,\n",
      "         -54.8269,  -56.9341,  -36.0117, -251.3287,  -56.9461,  -56.5720,\n",
      "         -38.0833,  -57.0804,  -54.4581,  -38.8893,  -51.7513,  -38.8386,\n",
      "         -56.2070, -341.4474,  -56.0771, -251.2543,  -51.6817,  -57.9703,\n",
      "         -36.2791, -181.5022], device='cuda:0', grad_fn=<SumBackward1>)\n",
      "tensor([   0.0000,    0.0000,  -54.2500, -552.0000,    0.0000, -580.0000,\n",
      "           0.0000,    0.0000,  -55.0000, -197.0000, -251.0000, -378.0000,\n",
      "         -56.7500,  -54.2500,    0.0000, -251.0000,  -57.7500,  -53.7500,\n",
      "           0.0000,  -55.5000,  -53.7500,    0.0000,  -52.5000,    0.0000,\n",
      "         -56.5000, -334.0000,  -56.2500, -251.0000,  -52.2500,  -55.5000,\n",
      "           0.0000,    0.0000], device='cuda:0')\n",
      "tensor([1.0000e+00, 1.0000e+00, 1.0995e+00, 3.9567e-03, 1.0000e+00, 4.3699e-04,\n",
      "        1.0000e+00, 1.0000e+00, 3.1480e+00, 1.1604e+00, 4.4504e-01, 1.5067e-01,\n",
      "        5.0000e+00, 6.8283e-02, 1.0000e+00, 7.1983e-01, 2.2343e+00, 5.9489e-02,\n",
      "        1.0000e+00, 2.0589e-01, 4.9256e-01, 1.0000e+00, 2.1142e+00, 1.0000e+00,\n",
      "        1.3404e+00, 5.8296e-04, 1.1887e+00, 7.7548e-01, 1.7652e+00, 8.4555e-02,\n",
      "        1.0000e+00, 1.0000e+00], device='cuda:0')\n",
      "tensor([ -55.8638,  -39.1580, -187.4591,  -40.1022,  -59.2429, -219.9958,\n",
      "         -57.4026, -394.3290,  -57.9961, -196.6440, -587.2844,  -59.6955,\n",
      "         -39.0162, -392.9311,  -58.2401, -560.0133, -282.7977, -143.8249,\n",
      "        -268.4142, -300.1352, -580.1399,  -58.7232,  -53.1948, -231.5448,\n",
      "        -379.0645, -340.5746, -169.3509,  -54.6771, -404.1692, -220.3389,\n",
      "         -59.0055, -219.8639], device='cuda:0', grad_fn=<SumBackward1>)\n",
      "tensor([ -55.0000,    0.0000,    0.0000,    0.0000,  -56.5000,    0.0000,\n",
      "         -57.0000,    0.0000,  -55.0000,    0.0000, -580.0000,  -57.0000,\n",
      "           0.0000,    0.0000,  -55.0000, -552.0000, -280.0000,    0.0000,\n",
      "        -266.0000, -300.0000,    0.0000,  -57.2500,  -53.0000,    0.0000,\n",
      "        -378.0000, -334.0000,    0.0000,  -53.0000,    0.0000, -217.0000,\n",
      "           0.0000,    0.0000], device='cuda:0')\n",
      "tensor([4.2154e-01, 1.0000e+00, 1.0000e+00, 1.0000e+00, 6.4385e-02, 1.0000e+00,\n",
      "        6.6858e-01, 1.0000e+00, 4.9982e-02, 1.0000e+00, 6.8614e-04, 6.7510e-02,\n",
      "        1.0000e+00, 1.0000e+00, 3.9158e-02, 3.3103e-04, 6.0948e-02, 1.0000e+00,\n",
      "        8.9440e-02, 8.7355e-01, 1.0000e+00, 2.2919e-01, 8.2304e-01, 1.0000e+00,\n",
      "        3.4492e-01, 1.3954e-03, 1.0000e+00, 1.8692e-01, 1.0000e+00, 3.5476e-02,\n",
      "        1.0000e+00, 1.0000e+00], device='cuda:0')\n",
      "tensor([-659.8727, -205.7700, -142.7224, -224.1022,  -60.2284, -292.2428,\n",
      "        -249.1796,  -55.1613,  -57.6707,  -57.9449,  -56.6647, -179.1106,\n",
      "         -57.9464,  -57.4246,  -42.3526, -620.4549, -179.1802,  -59.6328,\n",
      "         -55.6629,  -59.7709, -271.3085, -162.1665,  -75.4221,  -41.3365,\n",
      "        -631.0964, -205.2968, -391.4552, -179.1080,  -62.6855, -248.7495,\n",
      "        -123.3344, -557.2259], device='cuda:0', grad_fn=<SumBackward1>)\n",
      "tensor([-664.0000,    0.0000,    0.0000,    0.0000,  -54.5000, -294.0000,\n",
      "           0.0000,    0.0000,  -53.5000,  -53.5000,  -53.5000, -185.0000,\n",
      "         -54.2500,  -54.7500,    0.0000,    0.0000, -181.0000,    0.0000,\n",
      "         -52.7500,  -55.5000,    0.0000,    0.0000,    0.0000,    0.0000,\n",
      "           0.0000, -207.0000, -390.0000, -180.0000,    0.0000,    0.0000,\n",
      "           0.0000,    0.0000], device='cuda:0')\n",
      "tensor([5.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 3.2523e-03, 5.0000e+00,\n",
      "        1.0000e+00, 1.0000e+00, 1.5442e-02, 1.1738e-02, 4.2226e-02, 5.0000e+00,\n",
      "        2.4811e-02, 6.8937e-02, 1.0000e+00, 1.0000e+00, 5.0000e+00, 1.0000e+00,\n",
      "        5.4318e-02, 1.3969e-02, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
      "        1.0000e+00, 5.0000e+00, 2.3335e-01, 2.4399e+00, 1.0000e+00, 1.0000e+00,\n",
      "        1.0000e+00, 1.0000e+00], device='cuda:0')\n",
      "tensor([ -56.1564,  -41.7239, -252.4542, -176.8898,  -60.0368, -251.9274,\n",
      "         -61.6660, -630.9555,  -57.8244, -179.0663,  -58.2532,  -41.9395,\n",
      "        -271.8358,  -56.3441,  -53.6533,  -41.1630, -292.0883, -179.0413,\n",
      "         -56.2145, -195.0406,  -55.9612,  -61.6608, -248.9016,  -59.6540,\n",
      "        -577.7406, -554.9131,  -42.4016,  -43.2663,  -61.5074,  -41.4750,\n",
      "        -658.7758,  -59.5001], device='cuda:0', grad_fn=<SumBackward1>)\n",
      "tensor([ -50.2500,    0.0000,    0.0000, -178.0000,  -54.2500, -249.0000,\n",
      "         -55.2500,    0.0000,  -53.5000,    0.0000,    0.0000,    0.0000,\n",
      "           0.0000,  -51.0000,  -52.5000,    0.0000,    0.0000, -181.0000,\n",
      "           0.0000,    0.0000,    0.0000,  -55.0000, -251.0000,    0.0000,\n",
      "           0.0000, -552.0000,    0.0000,    0.0000,  -56.0000,    0.0000,\n",
      "        -644.0000,  -53.5000], device='cuda:0')\n",
      "tensor([2.7218e-03, 1.0000e+00, 1.0000e+00, 3.0349e+00, 3.0679e-03, 5.3536e-02,\n",
      "        1.6352e-03, 1.0000e+00, 1.3242e-02, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
      "        1.0000e+00, 4.7764e-03, 3.1559e-01, 1.0000e+00, 1.0000e+00, 5.0000e+00,\n",
      "        1.0000e+00, 1.0000e+00, 1.0000e+00, 1.2801e-03, 5.0000e+00, 1.0000e+00,\n",
      "        1.0000e+00, 5.4305e-02, 1.0000e+00, 1.0000e+00, 4.0565e-03, 1.0000e+00,\n",
      "        3.8278e-07, 2.4785e-03], device='cuda:0')\n",
      "tensor([ -56.6119,  -57.5455,  -62.7796,  -41.6225, -229.1813, -142.3577,\n",
      "         -59.4517,  -41.9024, -177.1936, -391.5076,  -60.1872, -252.3886,\n",
      "         -41.3552, -250.0920, -249.2414, -202.0932, -249.5366,  -58.0697,\n",
      "         -55.7614,  -53.0214,  -58.1826,  -56.0930, -229.7134, -179.4055,\n",
      "         -56.5114,  -59.5014, -339.8112, -504.5761,  -50.3625, -585.0146,\n",
      "         -59.8399,  -54.3654], device='cuda:0', grad_fn=<SumBackward1>)\n",
      "tensor([ -52.7500,  -53.2500,  -56.5000,    0.0000,    0.0000,    0.0000,\n",
      "         -57.5000,    0.0000, -178.0000, -390.0000,  -55.0000, -249.0000,\n",
      "           0.0000, -251.0000, -251.0000,    0.0000, -251.0000,  -55.2500,\n",
      "         -52.2500,  -52.0000,  -52.5000,    0.0000,    0.0000,    0.0000,\n",
      "           0.0000,  -54.7500, -334.0000, -496.0000,    0.0000, -580.0000,\n",
      "         -54.0000,  -51.7500], device='cuda:0')\n",
      "tensor([2.1029e-02, 1.3630e-02, 1.8741e-03, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
      "        1.4204e-01, 1.0000e+00, 2.2399e+00, 2.2145e-01, 5.5877e-03, 3.3756e-02,\n",
      "        1.0000e+00, 2.4794e+00, 5.0000e+00, 1.0000e+00, 4.3205e+00, 5.9621e-02,\n",
      "        2.9855e-02, 3.6011e-01, 3.4047e-03, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
      "        1.0000e+00, 8.6393e-03, 2.9940e-03, 1.8855e-04, 1.0000e+00, 6.6400e-03,\n",
      "        2.9091e-03, 7.3139e-02], device='cuda:0')\n",
      "tensor([-392.9163,  -61.8976, -392.2253, -203.3301,  -59.3886,  -58.9204,\n",
      "         -60.7895,  -58.1140,  -42.2244, -219.4834,  -55.4968,  -40.3639,\n",
      "         -57.7931,  -39.0549,  -61.8822, -392.7336,  -58.6751,  -61.4710,\n",
      "         -58.9264, -272.7484,  -60.4348,  -60.8557,  -43.5711, -246.9181,\n",
      "         -39.8290,  -39.5816, -423.2915,  -43.2968, -272.7976, -207.5240,\n",
      "         -61.5538,  -59.3679], device='cuda:0', grad_fn=<SumBackward1>)\n",
      "tensor([   0.0000,    0.0000,    0.0000, -201.0000,  -54.0000,  -52.2500,\n",
      "         -54.5000,  -54.2500,    0.0000, -217.0000,    0.0000,    0.0000,\n",
      "         -55.2500,    0.0000,    0.0000,    0.0000,    0.0000,  -55.0000,\n",
      "           0.0000,    0.0000,  -55.0000,  -54.2500,    0.0000, -249.0000,\n",
      "           0.0000,    0.0000,    0.0000,    0.0000,    0.0000, -207.0000,\n",
      "         -56.2500,  -55.2500], device='cuda:0')\n",
      "tensor([1.0000e+00, 1.0000e+00, 1.0000e+00, 9.7285e-02, 4.5685e-03, 1.2679e-03,\n",
      "        1.8556e-03, 2.0984e-02, 1.0000e+00, 8.3462e-02, 1.0000e+00, 1.0000e+00,\n",
      "        7.8623e-02, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.5477e-03,\n",
      "        1.0000e+00, 1.0000e+00, 4.3620e-03, 1.3526e-03, 1.0000e+00, 5.0000e+00,\n",
      "        1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 5.9216e-01,\n",
      "        4.9728e-03, 1.6279e-02], device='cuda:0')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-294.4225, -557.9563,  -59.8030,  -59.3786,  -60.8472,  -58.5339,\n",
      "        -252.3942,  -60.1437, -341.9130,  -40.2898,  -59.0742, -420.9152,\n",
      "         -61.0883,  -63.6726,  -57.0076, -415.5738,  -39.6594,  -41.0366,\n",
      "         -54.9229, -587.7743,  -57.1811,  -57.0502, -587.4235,  -57.8369,\n",
      "        -160.4869, -300.9436, -269.2845, -504.3029, -634.9478, -230.5892,\n",
      "        -273.6657, -380.0076], device='cuda:0', grad_fn=<SumBackward1>)\n",
      "tensor([-294.0000, -552.0000,    0.0000,    0.0000,    0.0000,    0.0000,\n",
      "           0.0000,    0.0000, -334.0000,    0.0000,  -58.7500, -416.0000,\n",
      "         -55.0000,  -57.7500,  -54.0000,    0.0000,    0.0000,    0.0000,\n",
      "         -52.2500,    0.0000,  -53.7500,  -54.5000,    0.0000,  -52.5000,\n",
      "           0.0000,    0.0000, -266.0000, -492.0000,    0.0000,    0.0000,\n",
      "        -268.0000, -378.0000], device='cuda:0')\n",
      "tensor([6.5542e-01, 2.5895e-03, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
      "        1.0000e+00, 1.0000e+00, 3.6595e-04, 1.0000e+00, 7.2314e-01, 7.3345e-03,\n",
      "        2.2694e-03, 2.6784e-03, 4.9411e-02, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
      "        6.9049e-02, 1.0000e+00, 3.2350e-02, 7.8070e-02, 1.0000e+00, 4.8108e-03,\n",
      "        1.0000e+00, 1.0000e+00, 3.7459e-02, 4.5388e-06, 1.0000e+00, 1.0000e+00,\n",
      "        3.4626e-03, 1.3431e-01], device='cuda:0')\n",
      "tensor([-268.7093,  -54.8065, -415.8424,  -38.0266, -232.4913, -208.0507,\n",
      "         -61.4605,  -55.1370, -231.5838, -393.6000,  -55.0035, -280.1755,\n",
      "        -181.0954, -230.6867,  -41.6880, -352.3079, -180.8991, -124.5451,\n",
      "         -55.3598, -149.9314, -180.5620,  -61.4147, -207.6278, -660.6398,\n",
      "        -579.6516,  -38.5564,  -58.8908, -505.9790, -178.7317,  -39.0577,\n",
      "         -60.3153,  -39.5837], device='cuda:0', grad_fn=<SumBackward1>)\n",
      "tensor([-266.0000,    0.0000, -408.0000,    0.0000,    0.0000, -207.0000,\n",
      "           0.0000,  -53.2500,    0.0000,    0.0000,    0.0000, -280.0000,\n",
      "        -181.0000,    0.0000,    0.0000, -346.0000,    0.0000,    0.0000,\n",
      "         -55.2500,    0.0000,    0.0000,  -55.7500,    0.0000,    0.0000,\n",
      "           0.0000,    0.0000,  -54.7500,    0.0000, -178.0000,    0.0000,\n",
      "           0.0000,    0.0000], device='cuda:0')\n",
      "tensor([6.6582e-02, 1.0000e+00, 3.9273e-04, 1.0000e+00, 1.0000e+00, 3.4970e-01,\n",
      "        1.0000e+00, 1.5152e-01, 1.0000e+00, 1.0000e+00, 1.0000e+00, 8.3901e-01,\n",
      "        9.0900e-01, 1.0000e+00, 1.0000e+00, 1.8218e-03, 1.0000e+00, 1.0000e+00,\n",
      "        8.9605e-01, 1.0000e+00, 1.0000e+00, 3.4661e-03, 1.0000e+00, 1.0000e+00,\n",
      "        1.0000e+00, 1.0000e+00, 1.5910e-02, 1.0000e+00, 4.8110e-01, 1.0000e+00,\n",
      "        1.0000e+00, 1.0000e+00], device='cuda:0')\n",
      "tensor([ -39.6542,  -56.6815,  -57.2008,  -38.9194, -219.3934, -224.0838,\n",
      "         -54.5057, -279.6633,  -55.8756, -247.5697, -415.8650, -180.5962,\n",
      "        -132.4332,  -57.8511,  -56.8887, -208.1271, -273.1611,  -56.6919,\n",
      "        -253.8083,  -55.4954, -279.4518,  -58.7182,  -55.3352,  -55.2527,\n",
      "        -378.9551,  -52.9332, -253.7394, -633.7759, -287.2404, -142.6388,\n",
      "        -251.3713, -139.8349], device='cuda:0', grad_fn=<SumBackward1>)\n",
      "tensor([   0.0000,    0.0000,  -55.0000,    0.0000,    0.0000,    0.0000,\n",
      "         -52.5000, -280.0000,  -54.0000,    0.0000, -408.0000,    0.0000,\n",
      "           0.0000,  -55.0000,    0.0000,    0.0000, -268.0000,    0.0000,\n",
      "        -249.0000,    0.0000, -280.0000,    0.0000,  -53.5000,  -54.2500,\n",
      "        -378.0000,  -53.2500,    0.0000,    0.0000,    0.0000,    0.0000,\n",
      "        -251.0000,    0.0000], device='cuda:0')\n",
      "tensor([1.0000e+00, 1.0000e+00, 1.1072e-01, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
      "        1.3457e-01, 1.4004e+00, 1.5326e-01, 1.0000e+00, 3.8395e-04, 1.0000e+00,\n",
      "        1.0000e+00, 5.7781e-02, 1.0000e+00, 1.0000e+00, 5.7354e-03, 1.0000e+00,\n",
      "        8.1614e-03, 1.0000e+00, 1.7302e+00, 1.0000e+00, 1.5959e-01, 3.6690e-01,\n",
      "        3.8477e-01, 1.3728e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
      "        6.8981e-01, 1.0000e+00], device='cuda:0')\n",
      "tensor([ -54.6918,  -55.5640,  -38.5241,  -56.2726, -253.3573,  -35.6957,\n",
      "         -37.0566, -391.0341,  -54.8473,  -62.4891,  -38.1614, -584.1831,\n",
      "         -52.9084, -250.0173, -271.6777, -281.7150,  -55.4310, -340.5347,\n",
      "         -56.0183, -246.7431,  -57.7834, -196.0146, -340.4374, -250.5369,\n",
      "         -56.4423, -321.5660, -167.8276,  -55.8825, -144.6571,  -53.7251,\n",
      "         -59.2819, -169.1163], device='cuda:0', grad_fn=<SumBackward1>)\n",
      "tensor([ -53.5000,    0.0000,    0.0000,    0.0000, -249.0000,    0.0000,\n",
      "           0.0000,    0.0000,  -53.7500,  -57.2500,    0.0000, -580.0000,\n",
      "           0.0000,    0.0000,    0.0000, -280.0000,    0.0000,    0.0000,\n",
      "         -56.5000, -249.0000,  -56.2500,    0.0000, -334.0000, -251.0000,\n",
      "         -56.2500,    0.0000,    0.0000,  -54.7500,    0.0000,  -52.5000,\n",
      "         -57.7500,    0.0000], device='cuda:0')\n",
      "tensor([3.0366e-01, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.2813e-02, 1.0000e+00,\n",
      "        1.0000e+00, 1.0000e+00, 3.3376e-01, 5.3050e-03, 1.0000e+00, 1.5251e-02,\n",
      "        1.0000e+00, 1.0000e+00, 1.0000e+00, 1.7996e-01, 1.0000e+00, 1.0000e+00,\n",
      "        1.6189e+00, 5.0000e+00, 2.1581e-01, 1.0000e+00, 1.6005e-03, 1.5890e+00,\n",
      "        8.2508e-01, 1.0000e+00, 1.0000e+00, 3.2224e-01, 1.0000e+00, 2.9374e-01,\n",
      "        2.1612e-01, 1.0000e+00], device='cuda:0')\n",
      "tensor([-101.9763, -252.0641,  -57.9405,  -55.5050,  -55.4037,  -53.0850,\n",
      "        -229.0358,  -38.9369, -338.7375,  -38.0150, -277.3682,  -37.6167,\n",
      "        -179.3647, -194.3780,  -59.0652, -338.9044, -228.4747,  -57.1972,\n",
      "        -499.6405, -178.9657,  -39.5919,  -55.1756,  -52.2449, -205.8393,\n",
      "        -180.3469,  -55.7436, -208.6866, -207.7856, -217.8518, -159.5726,\n",
      "         -36.6901,  -57.9619], device='cuda:0', grad_fn=<SumBackward1>)\n",
      "tensor([   0.0000, -249.0000,    0.0000,    0.0000,    0.0000,  -53.2500,\n",
      "        -233.0000,    0.0000, -334.0000,    0.0000, -280.0000,    0.0000,\n",
      "        -181.0000,    0.0000,  -56.2500, -334.0000,    0.0000,  -54.5000,\n",
      "        -492.0000, -181.0000,    0.0000,  -55.2500,  -53.0000, -207.0000,\n",
      "        -185.0000,    0.0000,    0.0000,    0.0000, -217.0000,    0.0000,\n",
      "           0.0000,  -55.7500], device='cuda:0')\n",
      "tensor([1.0000e+00, 4.6698e-02, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.1794e+00,\n",
      "        5.0000e+00, 1.0000e+00, 8.7604e-03, 1.0000e+00, 5.0000e+00, 1.0000e+00,\n",
      "        5.0000e+00, 1.0000e+00, 5.9892e-02, 7.4135e-03, 1.0000e+00, 6.7395e-02,\n",
      "        4.8059e-04, 5.0000e+00, 1.0000e+00, 1.0772e+00, 2.1279e+00, 3.1923e+00,\n",
      "        5.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 4.2664e-01, 1.0000e+00,\n",
      "        1.0000e+00, 1.0949e-01], device='cuda:0')\n",
      "tensor([-194.4518,  -36.2032,  -55.3955, -250.2542, -178.8924,  -53.3101,\n",
      "        -299.1750, -228.6347,  -54.3505,  -48.4446,  -71.7860, -242.3561,\n",
      "        -179.9515,  -53.5963,  -54.9162, -388.8307,  -53.2407, -194.7671,\n",
      "         -36.8310,  -35.1358, -217.4796, -552.1303,  -55.6460, -179.3178,\n",
      "        -227.4320, -349.5292, -249.8754,  -35.9012, -221.6799,  -53.5859,\n",
      "         -53.7363, -179.1221], device='cuda:0', grad_fn=<SumBackward1>)\n",
      "tensor([   0.0000,    0.0000,  -54.7500, -251.0000, -181.0000,    0.0000,\n",
      "        -300.0000,    0.0000,  -54.0000,  -50.5000,    0.0000,    0.0000,\n",
      "           0.0000,    0.0000,  -54.5000,    0.0000,    0.0000,    0.0000,\n",
      "           0.0000,    0.0000,    0.0000, -552.0000,  -56.7500, -185.0000,\n",
      "           0.0000, -346.0000, -251.0000,    0.0000,    0.0000,  -53.2500,\n",
      "         -54.2500, -181.0000], device='cuda:0')\n",
      "tensor([1.0000, 1.0000, 0.5244, 2.1080, 5.0000, 1.0000, 2.2818, 1.0000, 0.7043,\n",
      "        5.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.6596, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 0.8778, 3.0161, 5.0000, 1.0000, 0.0293, 3.0791,\n",
      "        1.0000, 1.0000, 0.7147, 1.6714, 5.0000], device='cuda:0')\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[52], line 117\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    108\u001b[0m trainer \u001b[38;5;241m=\u001b[39m IPSReinforceTrainer(\n\u001b[1;32m    109\u001b[0m model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m    110\u001b[0m args\u001b[38;5;241m=\u001b[39margs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    113\u001b[0m data_collator\u001b[38;5;241m=\u001b[39mdata_collator,\n\u001b[1;32m    114\u001b[0m     )\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStart training\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 117\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;66;03m# Save explicitly\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSave the model\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:2238\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2236\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2237\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2238\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2239\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2241\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2243\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:2582\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2575\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2576\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[1;32m   2577\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2578\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[1;32m   2579\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[1;32m   2580\u001b[0m )\n\u001b[1;32m   2581\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m-> 2582\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2584\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2585\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2586\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2587\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2588\u001b[0m ):\n\u001b[1;32m   2589\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2590\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:3845\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3842\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m==\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED:\n\u001b[1;32m   3843\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscale_wrt_gas\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m-> 3845\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3847\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/accelerate/accelerator.py:2734\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2732\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n\u001b[1;32m   2733\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2734\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py:648\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    639\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    640\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    641\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    646\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    647\u001b[0m     )\n\u001b[0;32m--> 648\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    650\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:353\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    348\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    350\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 353\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/graph.py:824\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    822\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    823\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 824\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    825\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    826\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    828\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbac8728",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.is_available())\n",
    "\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.device(0))\n",
    "print(torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b90a5b",
   "metadata": {},
   "source": [
    "Tried out the losses without IPS coefficient, with the normal loss, and these were the results.\n",
    "\n",
    "Step \tTraining Loss\n",
    "1 \t518.321400\n",
    "2 \t406.030500\n",
    "3 \t-916.910800\n",
    "4 \t-245.761400\n",
    "5 \t-996.767900\n",
    "6 \t-439.983500\n",
    "7 \t27.368200\n",
    "8 \t-303.980600\n",
    "9 \t98.382900\n",
    "10 \t1272.444300\n",
    "11 \t581.502000\n",
    "12 \t-508.267100\n",
    "13 \t-1107.622900\n",
    "14 \t-2049.277300\n",
    "15 \t715.464200\n",
    "16 \t97.236000\n",
    "17 \t-1123.329300\n",
    "18 \t734.981900\n",
    "19 \t-921.665000\n",
    "20 \t2865.062700\n",
    "21 \t109.311600\n",
    "22 \t-711.254700\n",
    "23 \t-748.666000\n",
    "24 \t-454.268800\n",
    "25 \t-2073.575900\n",
    "26 \t-1976.982200\n",
    "27 \t-246.352200\n",
    "28 \t-2102.958000\n",
    "29 \t-2487.609900\n",
    "30 \t704.824800\n",
    "31 \t-836.676900\n",
    "32 \t-1319.440900\n",
    "33 \t-2682.269300\n",
    "\n",
    "\n",
    "there is MOST DEFENITELY an issue with my loss somewher and I need to calmly debug it ... UFFF T_T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0230f1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See if another process is using the GPU\n",
    "!nvidia-smi\n",
    "\n",
    "\n",
    "# Check driver and CUDA versions\n",
    "#nvidia-smi | head -n 3\n",
    "#python -c \"import torch; print(torch.version.cuda); print(torch.backends.cudnn.version())\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79238b41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31efc040",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myclem)",
   "language": "python",
   "name": "myclem"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
